{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrends\n",
      "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytrends) (2.31.0)\n",
      "Requirement already satisfied: pandas>=0.25 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytrends) (2.2.1)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pytrends) (5.3.0)\n",
      "Requirement already satisfied: numpy<2,>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=0.25->pytrends) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas>=0.25->pytrends) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.0->pytrends) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.0->pytrends) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.0->pytrends) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.0->pytrends) (2024.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
      "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pytrends\n",
      "Successfully installed pytrends-4.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Collecting google-api-python-client\n",
      "  Downloading google_api_python_client-2.153.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client)\n",
      "  Downloading httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 (from google-api-python-client)\n",
      "  Downloading google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 (from google-api-python-client)\n",
      "  Downloading google_api_core-2.23.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.3)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client)\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.1.2)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Downloading google_api_python_client-2.153.0-py2.py3-none-any.whl (12.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.23.0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m156.6/156.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m209.5/209.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m96.9/96.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.66.0-py2.py3-none-any.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m221.7/221.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m83.1/83.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uritemplate, pyasn1, proto-plus, httplib2, googleapis-common-protos, cachetools, rsa, pyasn1-modules, google-auth, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed cachetools-5.5.0 google-api-core-2.23.0 google-api-python-client-2.153.0 google-auth-2.36.0 google-auth-httplib2-0.2.0 googleapis-common-protos-1.66.0 httplib2-0.22.0 proto-plus-1.25.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 rsa-4.9 uritemplate-4.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytrends\n",
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "# ÏÖÄÎ†àÎãàÏõÄ ÏÇ¨Ïö©\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException, NoSuchElementException, TimeoutException\n",
    "import time\n",
    "# Íµ¨Í∏Ä Ìä∏ÎûúÎìú ÏÇ¨Ïö©\n",
    "from pytrends.request import TrendReq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_partial_results(data_dict, data_col_name, filename=\"partial.csv\"):\n",
    "    \"\"\"\n",
    "    Ï§ëÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ìï®Ïàò Ï†ïÏùò\n",
    "    \"\"\"\n",
    "    partial_data = {\n",
    "        'year': list(data_dict.keys()),\n",
    "        data_col_name: list(data_dict.values()),\n",
    "    }\n",
    "    # Pandas DataFrameÏúºÎ°ú Î≥ÄÌôò ÌõÑ Ï†ÄÏû•\n",
    "    pd.DataFrame(partial_data).to_csv(filename, index=False)\n",
    "    print(f\"Î∂ÄÎ∂Ñ Îç∞Ïù¥ÌÑ∞Í∞Ä {filename}Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\")\n",
    "\n",
    "def get_article_counts_by_year():\n",
    "    '''\n",
    "    Ïó∞ÎèÑÎ≥ÑÎ°ú ÌîÑÎ°úÏïºÍµ¨ Í¥ÄÎ†® Í∏∞ÏÇ¨ Í∞úÏàòÎ•º ÌÅ¨Î°§ÎßÅÌïúÎã§.\n",
    "    Î¶¨ÌÑ¥Í∞í: 2010ÎÖÑÎ∂ÄÌÑ∞ 2024ÎÖÑÍπåÏßÄ Í∏∞ÏÇ¨ Í∞úÏàòÎ•º ÎÖÑÎèÑ: Í∏∞ÏÇ¨ Í∞úÏàò ÎîïÏÖîÎÑàÎ¶¨ ÌòïÌÉúÎ°ú Î¶¨ÌÑ¥\n",
    "    '''\n",
    "    total_articles_counts = {year: 0 for year in range(2010, 2025)}\n",
    "    for year in range(2010, 2025):\n",
    "        for month in range(1, 13):\n",
    "            for day in range(1, 32):\n",
    "                # ÎÇ†Ïßú ÌòïÏãù ÎßûÏ∂îÍ∏∞ (Ïòà: 20240101)\n",
    "                date = f\"{year}{month:02}{day:02}\"\n",
    "\n",
    "                # ÎÇ†Ïßú Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨\n",
    "                try:\n",
    "                    time.strptime(date, \"%Y%m%d\")  # Ïú†Ìö®ÌïòÏßÄ ÏïäÏùÄ ÎÇ†ÏßúÎäî ValueError Î∞úÏÉù\n",
    "                except ValueError:\n",
    "                    continue\n",
    "\n",
    "                # ÌïòÎ£® Í∏∞ÏÇ¨ Í∞úÏàò Í≥ÑÏÇ∞\n",
    "                daily_articles = get_daily_article_count(date)\n",
    "                total_articles_counts[year] += daily_articles\n",
    "                print(f\"{date} Í∏∞ÏÇ¨ Í∞úÏàò: {daily_articles}Í∞ú / Ï¥ù: {total_articles_counts[year]}Í∞ú\")\n",
    "        save_partial_results(total_articles_counts, 'article_counts_by_year', f\"{year}_partial_y_aud_arti.csv\")\n",
    "    return total_articles_counts\n",
    "\n",
    "def get_daily_article_count(date):\n",
    "    '''\n",
    "    ÌïòÎ£®Ïùò Í∏∞ÏÇ¨ Í∞úÏàòÎ•º Î¶¨ÌÑ¥Ìï©ÎãàÎã§.\n",
    "    Î¶¨ÌÑ¥Í∞í: int\n",
    "    '''\n",
    "    total_post_counts = 0\n",
    "    # Selenium Chromedriver ÏÑ§Ï†ï\n",
    "    chromedriver_path = \"/usr/local/bin/chromedriver\"  # Chromedriver Í≤ΩÎ°ú\n",
    "    service = Service(chromedriver_path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    # ÎÑ§Ïù¥Î≤Ñ ÏïºÍµ¨ ÌéòÏù¥ÏßÄ Ïù¥Îèô\n",
    "    base_url = f\"https://sports.news.naver.com/kbaseball/news/index?isphoto=N&view=photo&date={date}&type=latest\"\n",
    "    try: \n",
    "        driver.get(base_url)\n",
    "        time.sleep(5)  # ÌéòÏù¥ÏßÄ Î°úÎî© ÎåÄÍ∏∞\n",
    "\n",
    "        # Ï†ÑÏ≤¥ ÌéòÏù¥ÏßÄ Ïàò Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        try:\n",
    "            while True:\n",
    "                total_pages_element = driver.find_element(By.CSS_SELECTOR, \"div.paginate\")\n",
    "                total_pages = total_pages_element.text.split()\n",
    "                print(total_pages)\n",
    "                # ÎßàÏßÄÎßâ ÌéòÏù¥ÏßÄ Ï∞æÍ∏∞\n",
    "                if total_pages[-1] == \"Îã§Ïùå\":\n",
    "                    next_button = driver.find_element(By.CLASS_NAME, \"next\")\n",
    "                    next_button.click()\n",
    "                    time.sleep(2)\n",
    "                else: # ÎßàÏßÄÎßâ ÌéòÏù¥ÏßÄ Í∏∞ÏÇ¨ Í∞úÏàò + 30 * (ÎßàÏßÄÎßâ ÌéòÏù¥ÏßÄ Í∞úÏàò - 1)\n",
    "                    try: \n",
    "                        last_page_id = total_pages[-1]\n",
    "                        last_page_button = driver.find_element(By.CSS_SELECTOR, f\"a[data-id='{last_page_id}']\")\n",
    "                        last_page_button.click()\n",
    "                        time.sleep(2)\n",
    "                    except Exception as e:\n",
    "                        print(f\"{date}: {e}\")\n",
    "                        \n",
    "                        \n",
    "                    ul_selector = \"div.news_list2 > ul\"  # ul ÌÉúÍ∑∏Ïùò ÏÑ†ÌÉùÏûê\n",
    "                    li_selector = f\"{ul_selector} > li\"  # ul ÏïÑÎûò li ÏÑ†ÌÉùÏûê\n",
    "                    li_elements = driver.find_elements(By.CSS_SELECTOR, li_selector)\n",
    "                    total_post_counts = (int(last_page_id)-1) * 30 + len(li_elements)\n",
    "                    # print(f\"ul ÏïÑÎûò li Í∞úÏàò: {len(li_elements)}Í∞ú\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            print(f\"Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
    "\n",
    "        # ÎìúÎùºÏù¥Î≤Ñ Ï¢ÖÎ£å\n",
    "        driver.quit()\n",
    "        # print(f\"{date} Í∏∞ÏÇ¨ Í∞úÏàò: {total_post_counts}\")\n",
    "        return total_post_counts\n",
    "    except WebDriverException as e:\n",
    "        print(f\"URL Î°úÎìú Ï§ë Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
    "        return 0\n",
    "\n",
    "def get_web_search_by_year():\n",
    "    \"\"\"\n",
    "    Íµ¨Í∏Ä Ìä∏ÎûúÎìúÎ•º Ïù¥Ïö©Ìïú Ïó∞ÎèÑÎ≥Ñ Ïõπ Í≤ÄÏÉâÎüâÏùÑ ÌÅ¨Î°§ÎßÅÌïúÎã§.\n",
    "    Î¶¨ÌÑ¥Í∞í: 2010~2024ÎÖÑ Ïõπ Í≤ÄÏÉâÎüâÏùÑ {'year': Í≤ÄÏÉâÎüâ} ÌòïÌÉúÏùò ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôò.\n",
    "    \"\"\"\n",
    "    pytrends = TrendReq(hl=\"ko\", tz=360)\n",
    "    keywords = [\"ÏïºÍµ¨\", \"KBO\", \"ÏïºÍµ¨Ïû•\", \"ÌîÑÎ°úÏïºÍµ¨\"]\n",
    "\n",
    "    # ÏöîÏ≤≠ ÎîúÎ†àÏù¥ Î∞è ÏóêÎü¨ Î∞©ÏßÄ\n",
    "    try:\n",
    "        pytrends.build_payload(keywords, timeframe=\"2010-01-01 2024-12-31\", geo=\"KR\")\n",
    "        trends = pytrends.interest_over_time()\n",
    "    except Exception as e:\n",
    "        print(f\"Îç∞Ïù¥ÌÑ∞ ÏöîÏ≤≠ Ïã§Ìå®: {e}\")\n",
    "        return {}\n",
    "\n",
    "    # Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\n",
    "    if not trends.empty:\n",
    "        trends = trends.reset_index()  # ÎÇ†Ïßú(date)Î•º Ïó¥Î°ú Î≥ÄÌôò\n",
    "        trends[\"year\"] = trends[\"date\"].dt.year  # Ïó∞ÎèÑ Ï∂îÏ∂ú\n",
    "\n",
    "        # Ïó∞ÎèÑÎ≥Ñ Ìï©ÏÇ∞\n",
    "        yearly_trends = trends.groupby(\"year\")[[\"ÏïºÍµ¨\", \"KBO\", \"ÏïºÍµ¨Ïû•\", \"ÌîÑÎ°úÏïºÍµ¨\"]].sum()\n",
    "\n",
    "        # Î™®Îì† ÌÇ§ÏõåÎìúÏùò Ìï©ÏÇ∞ Ïó¥ Ï∂îÍ∞Ä\n",
    "        yearly_trends[\"web_search_by_years\"] = yearly_trends.sum(axis=1)\n",
    "\n",
    "        # Ïó∞ÎèÑÎ≥Ñ Ï¥ù Í≤ÄÏÉâÎüâ ÎîïÏÖîÎÑàÎ¶¨Î°ú Î∞òÌôò\n",
    "        web_search_counts = yearly_trends[\"web_search_by_years\"].to_dict()\n",
    "        return web_search_counts\n",
    "    else:\n",
    "        print(\"Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ ÏûàÏäµÎãàÎã§.\")\n",
    "        return {}\n",
    "\n",
    "# def get_kbo_youtube_views_by_year():\n",
    "#     '''\n",
    "#     Ïó∞ÎèÑÎ≥Ñ KBO Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàòÎ•º ÌÅ¨Î°§ÎßÅÌïúÎã§.\n",
    "#     Î¶¨ÌÑ¥Í∞í: ~2024ÎÖÑÍπåÏßÄ Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàòÎ•º Î¶¨Ïä§Ìä∏Î°ú Î¶¨ÌÑ¥\n",
    "#     '''\n",
    "\n",
    "# # Ï§ëÍ∞Ñ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ìï®Ïàò Ï†ïÏùò\n",
    "# def save_partial_results(likes_counts, post_counts, filename=\"partial_y_a_insta.csv\"):\n",
    "#     \"\"\"\n",
    "#     likes_countsÏôÄ post_counts Îç∞Ïù¥ÌÑ∞Î•º ÏßÄÏ†ïÎêú ÌååÏùºÏóê Ï†ÄÏû•Ìï©ÎãàÎã§.\n",
    "#     \"\"\"\n",
    "#     partial_data = {\n",
    "#         'year': list(likes_counts.keys()),\n",
    "#         'instagram_likes_by_year': list(likes_counts.values()),\n",
    "#         'instagram_posts_by_year': list(post_counts.values()),\n",
    "#     }\n",
    "#     # Pandas DataFrameÏúºÎ°ú Î≥ÄÌôò ÌõÑ Ï†ÄÏû•\n",
    "#     pd.DataFrame(partial_data).to_csv(filename, index=False)\n",
    "#     print(f\"Î∂ÄÎ∂Ñ Îç∞Ïù¥ÌÑ∞Í∞Ä {filename}Ïóê Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§!\")\n",
    "    \n",
    "def get_instagram_data_by_year():\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.webdriver.common.action_chains import ActionChains\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium import webdriver\n",
    "    import time\n",
    "\n",
    "    # Chromedriver Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "    chromedriver_path = \"/usr/local/bin/chromedriver\"\n",
    "    service = Service(chromedriver_path)\n",
    "    dr = webdriver.Chrome(service=service)\n",
    "    dr.set_window_size(414, 800)\n",
    "    dr.set_page_load_timeout(300)\n",
    "\n",
    "    # Instagram Î°úÍ∑∏Ïù∏\n",
    "    dr.get('https://www.instagram.com/')\n",
    "    time.sleep(2)\n",
    "    id_box = WebDriverWait(dr, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"#loginForm > div > div:nth-child(1) > div > label > input\")))\n",
    "    password_box = dr.find_element(By.CSS_SELECTOR, \"#loginForm > div > div:nth-child(2) > div > label > input\")\n",
    "    login_button = dr.find_element(By.CSS_SELECTOR, '#loginForm > div > div:nth-child(3) > button')\n",
    "    id_box.send_keys(\"sksohn01@sookmyung.ac.kr\")\n",
    "    password_box.send_keys(\"sks0hn01!!\")\n",
    "    login_button.click()\n",
    "    time.sleep(5)\n",
    "\n",
    "    # KBO Í≥ÑÏ†ïÏúºÎ°ú Ïù¥Îèô\n",
    "    dr.get(\"https://www.instagram.com/kbo.official/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Ï¢ãÏïÑÏöî Ïàò Î∞è Í≤åÏãúÎ¨º Ïàò Í≥ÑÏÇ∞\n",
    "    likes_counts = {year: 0 for year in range(2010, 2025)}\n",
    "    post_counts = {year: 0 for year in range(2010, 2025)}\n",
    "\n",
    "    # Î¨¥Ìïú Ïä§ÌÅ¨Î°§\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    last_height = dr.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        dr.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        new_height = dr.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Í≤åÏãúÎ¨º Ï≤òÎ¶¨\n",
    "    cards = dr.find_elements(By.CLASS_NAME, '_aagw')\n",
    "    for i in range(len(cards)):\n",
    "        retries = 0\n",
    "        while retries < 3:  # Ïû¨ÏãúÎèÑ ÏµúÎåÄ 3Ìöå\n",
    "            try:\n",
    "                cards = dr.find_elements(By.CLASS_NAME, '_aagw')  # Îß§Î≤à ÏöîÏÜå Ïû¨Ï∞∏Ï°∞\n",
    "                card = cards[i]\n",
    "\n",
    "                # Ïä§ÌÅ¨Î°§ ÌõÑ ÌÅ¥Î¶≠\n",
    "                dr.execute_script(\"arguments[0].scrollIntoView(true);\", card)\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Í∞ïÏ†ú ÌÅ¥Î¶≠\n",
    "                ActionChains(dr).move_to_element(card).click().perform()\n",
    "                time.sleep(2)\n",
    "\n",
    "                # Ï¢ãÏïÑÏöî Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨\n",
    "                like_element = WebDriverWait(dr, 30).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, 'x193iq5w.xeuugli.x1fj9vlw.x13faqbe.x1vvkbs.xt0psk2.x1i0vuye.xvs91rp.x1s688f.x10wh9bi.x1wdrske.x8viiok.x18hxmgj'))\n",
    "                )\n",
    "                likes = like_element.text.replace(\"Ï¢ãÏïÑÏöî \", \"\").replace(\"Í∞ú\", \"\").strip()\n",
    "                if \"Îßå\" in likes:\n",
    "                    likes = float(likes[:-1]) * 10000\n",
    "\n",
    "                # ÏóÖÎ°úÎìú Ïó∞ÎèÑ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "                time_element = WebDriverWait(dr, 30).until(EC.presence_of_element_located((By.CSS_SELECTOR, 'time.x1p4m5qa')))\n",
    "                upload_time = int(time_element.get_attribute('datetime')[:4])\n",
    "\n",
    "                # Îç∞Ïù¥ÌÑ∞ ÎàÑÏ†Å\n",
    "                likes_counts[upload_time] += int(likes)\n",
    "                post_counts[upload_time] += 1\n",
    "                break  # ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌñàÏúºÎ©¥ Ïû¨ÏãúÎèÑ Î£®ÌîÑ Ï¢ÖÎ£å\n",
    "            except Exception as e:\n",
    "                retries += 1\n",
    "                print(f\"Í≤åÏãúÎ¨º {i+1} ÌÅ¥Î¶≠ Ï§ë Ïò§Î•ò Î∞úÏÉù, Ïû¨ÏãúÎèÑ {retries}/3: {e}\")\n",
    "        else:\n",
    "            print(f\"Í≤åÏãúÎ¨º {i+1} Ï≤òÎ¶¨ Ïã§Ìå®, Îã§Ïùå Í≤åÏãúÎ¨ºÎ°ú ÎÑòÏñ¥Í∞ëÎãàÎã§.\")\n",
    "\n",
    "        # Îí§Î°úÍ∞ÄÍ∏∞\n",
    "        dr.back()\n",
    "        time.sleep(2)\n",
    "\n",
    "    dr.quit()\n",
    "    return likes_counts, post_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïù∏Í∏∞ÎèÑ ÏπºÎüº ÏàòÏßë: Í¥ÄÏ§ëÏàò(audience_by_year), Ïó∞ÎèÑÎ≥Ñ Í∏∞ÏÇ¨ Í∞úÏàò(article_counts_by_year), ÌÅ¨Î≥¥ Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàò(kbo_youtube_views_by_year), Ïù∏Ïä§ÌÉÄÍ∑∏Îû® Ìï¥ÏãúÌÉúÍ∑∏ Ïñ∏Í∏âÎüâ(instagram_hashtags_by_year), Î∞©ÏÜ° ÏãúÏ≤≠Î•†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïó∞ÎèÑÎ≥Ñ Í∏∞ÏÇ¨ Í∞úÏàò, Í¥ÄÏ§ëÏàò Íµ¨ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïó∞ÎèÑÎ≥Ñ Í∏∞ÏÇ¨ Í∞úÏàò, Í¥ÄÏ§ëÏàò Íµ¨ÌïòÍ∏∞\n",
    "################ !!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Ïã§Ìñâ Í∏àÏßÄ\n",
    "##############################################################\n",
    "year = [\n",
    "    2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019,\n",
    "    2020, 2021, 2022, 2023, 2024\n",
    "]\n",
    "audience_by_year = [\n",
    "    5928626, 6810028, 7156157, 6441945, 6509915, 7360530, 8339577, 8400688, 8073742, 7286008, \n",
    "    328317, 1228489, 6076074, 8100326, 10887705\n",
    "]\n",
    "\n",
    "article_counts = get_article_counts_by_year()\n",
    "data = {'year': year,\n",
    "        'audience_by_year': audience_by_year,\n",
    "        'article_counts_by_year': list(article_counts.values()),\n",
    "}\n",
    "year_data = pd.DataFrame(data)\n",
    "# year_data.to_csv('y_aud_arti.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïõπ Í≤ÄÏÉâÎüâ Îç∞Ïù¥ÌÑ∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    }
   ],
   "source": [
    "# Ïõπ Í≤ÄÏÉâÎüâ Îç∞Ïù¥ÌÑ∞\n",
    "data = pd.read_csv(\"./y_aud_arti.csv\")\n",
    "\n",
    "web_search_counts = get_web_search_by_year()\n",
    "data['web_search_count_by_year'] = list(web_search_counts.values())\n",
    "\n",
    "data = pd.DataFrame(data)\n",
    "data.to_csv('add_web_search.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàò, Ï¢ãÏïÑÏöîÏàò, ÎåìÍ∏ÄÏàò Íµ¨ÌïòÍ∏∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàò, Ï¢ãÏïÑÏöîÏàò, ÎåìÍ∏ÄÏàò Íµ¨ÌïòÍ∏∞\n",
    "\n",
    "data = pd.read_csv(\"./y_aud_arti.csv\")\n",
    "kbo_youtube_views = get_kbo_youtube_views_by_year()\n",
    "\n",
    "data['kbo_youtube_views_by_year'] = list(kbo_youtube_views.values())\n",
    "year_data = pd.DataFrame(data)\n",
    "year_data.to_csv('yaa_youtube.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> API key ÏÑ§Ï†ï\n",
      ">>> ÎåÄÏÉÅ Ï±ÑÎÑêÎ™Ö: KBO\n",
      ">>> ÎåÄÏÉÅ Ï±ÑÎÑê ID: UCoVz66yWHzVsXAFG8WhJK9g\n",
      ">>> YouTubeÏóêÏÑú Ìï¥Îãπ Ï±ÑÎÑêÏóê ÏÜçÌïú Î™®Îì† ÎπÑÎîîÏò§ ID ÌôïÏù∏ ÏôÑÎ£å\n",
      ">>> Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ï§ÄÎπÑ ÏôÑÎ£å\n",
      ">>> Í∞úÎ≥Ñ ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏãúÏûë\n",
      "{'kind': 'youtube#videoListResponse', 'etag': 'kf8ed2lEE8sOT59XaEv6ESOEf8s', 'items': [{'kind': 'youtube#video', 'etag': 'm0Sdqej9yztdpyLTsvzGCH7dV3A', 'id': 'aSx_Ie97Vhs', 'snippet': {'publishedAt': '2024-11-16T16:43:53Z', 'channelId': 'UCoVz66yWHzVsXAFG8WhJK9g', 'title': \"[ùó¢.ùóß.ùó•] '8ÌöåÏùò ÏïΩÏÜç? ÏïΩÏÜçÏùò 8Ìöå?' Ïñ¥Ï®åÎì† ÎßåÎì† Ï£ºÏù∏Í≥µ Ïù∏ÌÑ∞Î∑∞(Î∞ïÏÑ±Ìïú, ÏµúÏõêÏ§Ä, Î∞ïÏòÅÌòÑ)| 2024 WBSC ÌîÑÎ¶¨ÎØ∏Ïñ¥12 ÏïºÍµ¨ Íµ≠Í∞ÄÎåÄÌëú ÌÅ¨Î≥¥ÏßÅÏ∫†\", 'description': '#ÌîÑÎ¶¨ÎØ∏Ïñ¥12 #ÏïºÍµ¨ÎåÄÌëúÌåÄ #ÏïºÍµ¨ \\nÌåÄÏΩîÎ¶¨ÏïÑ ÏïºÍµ¨ ÎåÄÌëúÌåÄ ÏΩòÌÖêÏ∏† \"Ïò§ÌîÑ Îçî Î†àÏΩîÎìú! [O.T.R]\" \\nÎåÄÌïúÎØºÍµ≠ÏùÑ ÎåÄÌëúÌïòÎäî ÏÑ†ÏàòÎì§Ïùò ÎπÑÌïòÏù∏ÎìúÍ∞Ä Í∂ÅÍ∏àÌïòÏãúÎ©¥ Íµ¨ÎèÖüîîÍ≥º Ï¢ãÏïÑÏöîüëç Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§!\\n\\n\\n‚úâÔ∏è Í¥ëÍ≥†/Ïä§Ìè∞ÏÑú/ÌòëÏóÖ Î¨∏Ïùò\\ngh@koreabaseball.or.kr\\n\\nKBO Î¶¨Í∑∏Îäî Ïò§ÏßÅ TVINGÏóêÏÑú! üì±‚ú®\\n‚úâÔ∏è Í¥ëÍ≥†/Ïä§Ìè∞ÏÑú/ÌòëÏóÖ Î¨∏Ïùò\\ngh@koreabaseball.or.kr\\n\\nKBO Î¶¨Í∑∏Îäî Ïò§ÏßÅ TVINGÏóêÏÑú! üì±‚ú®', 'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/aSx_Ie97Vhs/default.jpg', 'width': 120, 'height': 90}, 'medium': {'url': 'https://i.ytimg.com/vi/aSx_Ie97Vhs/mqdefault.jpg', 'width': 320, 'height': 180}, 'high': {'url': 'https://i.ytimg.com/vi/aSx_Ie97Vhs/hqdefault.jpg', 'width': 480, 'height': 360}, 'standard': {'url': 'https://i.ytimg.com/vi/aSx_Ie97Vhs/sddefault.jpg', 'width': 640, 'height': 480}, 'maxres': {'url': 'https://i.ytimg.com/vi/aSx_Ie97Vhs/maxresdefault.jpg', 'width': 1280, 'height': 720}}, 'channelTitle': 'KBO', 'tags': ['ÏïºÍµ¨', 'kbo', 'Í∏∞ÏïÑÌÉÄÏù¥Í±∞Ï¶à', 'ÌïúÌôîÏù¥Í∏ÄÏä§', 'Î°ØÎç∞ÏûêÏù¥Ïñ∏Ï∏†', 'ÏóòÏßÄÌä∏ÏúàÏä§', 'ÎëêÏÇ∞Î≤†Ïñ¥Ïä§', 'ssgÎûúÎçîÏä§', 'ÏÇºÏÑ±ÎùºÏù¥Ïò®Ï¶à', 'ktÏúÑÏ¶à', 'ÌÇ§ÏõÄÌûàÏñ¥Î°úÏ¶à', 'ncÎã§Ïù¥ÎÖ∏Ïä§', 'kiaÌÉÄÏù¥Í±∞Ï¶à', 'lgÌä∏ÏúàÏä§', 'baseball', 'ÌïúÍµ≠ÏïºÍµ¨', 'ÌîÑÎ°úÏïºÍµ¨', 'ÌïòÏù¥ÎùºÏù¥Ìä∏', 'ÏïºÍµ¨Ïû•', 'Ìè¨Ïä§Ìä∏ÏãúÏ¶å'], 'categoryId': '17', 'liveBroadcastContent': 'none', 'defaultLanguage': 'ko', 'localized': {'title': \"[ùó¢.ùóß.ùó•] '8ÌöåÏùò ÏïΩÏÜç? ÏïΩÏÜçÏùò 8Ìöå?' Ïñ¥Ï®åÎì† ÎßåÎì† Ï£ºÏù∏Í≥µ Ïù∏ÌÑ∞Î∑∞(Î∞ïÏÑ±Ìïú, ÏµúÏõêÏ§Ä, Î∞ïÏòÅÌòÑ)| 2024 WBSC ÌîÑÎ¶¨ÎØ∏Ïñ¥12 ÏïºÍµ¨ Íµ≠Í∞ÄÎåÄÌëú ÌÅ¨Î≥¥ÏßÅÏ∫†\", 'description': '#ÌîÑÎ¶¨ÎØ∏Ïñ¥12 #ÏïºÍµ¨ÎåÄÌëúÌåÄ #ÏïºÍµ¨ \\nÌåÄÏΩîÎ¶¨ÏïÑ ÏïºÍµ¨ ÎåÄÌëúÌåÄ ÏΩòÌÖêÏ∏† \"Ïò§ÌîÑ Îçî Î†àÏΩîÎìú! [O.T.R]\" \\nÎåÄÌïúÎØºÍµ≠ÏùÑ ÎåÄÌëúÌïòÎäî ÏÑ†ÏàòÎì§Ïùò ÎπÑÌïòÏù∏ÎìúÍ∞Ä Í∂ÅÍ∏àÌïòÏãúÎ©¥ Íµ¨ÎèÖüîîÍ≥º Ï¢ãÏïÑÏöîüëç Î∂ÄÌÉÅÎìúÎ¶ΩÎãàÎã§!\\n\\n\\n‚úâÔ∏è Í¥ëÍ≥†/Ïä§Ìè∞ÏÑú/ÌòëÏóÖ Î¨∏Ïùò\\ngh@koreabaseball.or.kr\\n\\nKBO Î¶¨Í∑∏Îäî Ïò§ÏßÅ TVINGÏóêÏÑú! üì±‚ú®\\n‚úâÔ∏è Í¥ëÍ≥†/Ïä§Ìè∞ÏÑú/ÌòëÏóÖ Î¨∏Ïùò\\ngh@koreabaseball.or.kr\\n\\nKBO Î¶¨Í∑∏Îäî Ïò§ÏßÅ TVINGÏóêÏÑú! üì±‚ú®'}, 'defaultAudioLanguage': 'ko'}, 'contentDetails': {'duration': 'PT3M29S', 'dimension': '2d', 'definition': 'hd', 'caption': 'false', 'licensedContent': True, 'contentRating': {}, 'projection': 'rectangular'}, 'statistics': {'viewCount': '43016', 'likeCount': '1460', 'favoriteCount': '0', 'commentCount': '183'}}], 'pageInfo': {'totalResults': 1, 'resultsPerPage': 1}}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[146], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m         likes\u001b[38;5;241m.\u001b[39mappend(lc)\n\u001b[1;32m     94\u001b[0m         comments\u001b[38;5;241m.\u001b[39mappend(cc)\n\u001b[0;32m---> 95\u001b[0m         upload_date\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpA\u001b[49m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>> Í∞úÎ≥Ñ ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>> ÎπÑÎîîÏò§ URL Ï†ïÎ¶¨\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pA' is not defined"
     ]
    }
   ],
   "source": [
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "from googleapiclient.discovery import build\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "''' ÏïÑÎûò \"<--change here-->\" Ïóê ÏûêÏã†Ïùò API-KeyÎ•º ÏûÖÎ†•ÌïòÎ©¥ Îê©ÎãàÎã§.\n",
    "'''\n",
    "\n",
    "\n",
    "# API keyÏôÄ YouTube API Î≤ÑÏ†ÑÏùÑ ÏÑ∏ÌåÖ\n",
    "api_key = \"AIzaSyBpWqno1zpgTXq0xw2AxiPj3fCFyZkPRCQ\"\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "print('>>> API key ÏÑ§Ï†ï')\n",
    "\n",
    "# Ïú†ÌäúÎ∏å Î™®Îì† Ï±ÑÎÑêÏóêÎäî 'Uploads'ÎùºÎäî Í∏∞Î≥∏ Ï±ÑÎÑêÏù¥ ÏûàÏùå\n",
    "# Ìï¥Îãπ Ï±ÑÎÑê IDÎ•º Í∞ÄÏ†∏Ïò® Îí§Ïóê, Ìï¥Îãπ UploadsÏùò Î¶¨Ïä§Ìä∏Î•º Îã§Ïãú Ìò∏Ï∂úÌï¥Ïò§Îäî Ìï®Ïàò\n",
    "\n",
    "# Ïú†ÌäúÎ∏å Îç∞Ïù¥ÌÑ∞Îäê Îß§Ïùº Î≥ÄÍ≤ΩÎê®. Ïò§Îäò ÎÇ†ÏßúÎ•º Last_updateÎ°ú Í∏∞Î°ùÌïòÍ∏∞ ÏúÑÌï¥ DatetimeÏùÑ ÏÇ¨Ïö©\n",
    "today = datetime.datetime.now()\n",
    "nowDate = today.strftime('%Y-%m-%d')\n",
    "\n",
    "# Ï±ÑÎÑêidÎ•º Í∞ÄÏßÄÍ≥† ÎπÑÎîîÏò§ Î¶¨Ïä§Ìä∏Î•º Í∞ÄÏ†∏Ïò§Îäî Ìï®Ïàò\n",
    "def get_channel_videos(channel_id):\n",
    "    # get Uploads playlist id\n",
    "    res = youtube.channels().list(id=channel_id, part='contentDetails').execute()\n",
    "    playlist_id = res['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "    res2 = youtube.channels().list(id=channel_id, part='snippet').execute()\n",
    "    channel_title = res2['items'][0]['snippet']['title']\n",
    "    print('>>> ÎåÄÏÉÅ Ï±ÑÎÑêÎ™Ö: ' + channel_title)\n",
    "    videos = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while 1:\n",
    "        res = youtube.playlistItems().list(playlistId=playlist_id,\n",
    "                                           part='snippet',\n",
    "                                           maxResults=50,\n",
    "                                           pageToken=next_page_token).execute()\n",
    "        videos += res['items']\n",
    "        next_page_token = res.get('nextPageToken')\n",
    "\n",
    "        if next_page_token is None:\n",
    "            break\n",
    "\n",
    "    return videos\n",
    "\n",
    "# Ï±ÑÎÑêÏùò IDÎ•º ÏûÖÎ†•ÌïòÎ©¥, Í∑∏ Ï±ÑÎÑêÏóê ÏÜçÌïú ÎπÑÎîîÏò§Î•º Ï∂îÏ∂ú\n",
    "chan_id = input(\"Ï±ÑÎÑê ÏïÑÏù¥Îîî: \")\n",
    "videos = get_channel_videos(chan_id)\n",
    "\n",
    "print('>>> ÎåÄÏÉÅ Ï±ÑÎÑê ID: '+ chan_id)\n",
    "print('>>> YouTubeÏóêÏÑú Ìï¥Îãπ Ï±ÑÎÑêÏóê ÏÜçÌïú Î™®Îì† ÎπÑÎîîÏò§ ID ÌôïÏù∏ ÏôÑÎ£å')\n",
    "\n",
    "# Ï∂îÏ∂úÎêú ÎπÑÎîîÏò§ Î¶¨Ïä§Ìä∏ÏóêÏÑú video IDÎßåÏùÑ Ï∂îÏ∂úÌïòÏó¨ listÎ°ú ÎßåÎì†Îã§.\n",
    "videoid_list = []\n",
    "for video in videos:\n",
    "    id_from_api = video['snippet']['resourceId']['videoId']\n",
    "    videoid_list.append(id_from_api)\n",
    "\n",
    "# videoid_listÏóê IDÎßå Î™®Îëê Ï∂îÏ∂úÌïòÏó¨ Ï†ÄÏû•Ïù¥ ÎêòÏóàÎã§.\n",
    "\n",
    "# Í∞Å ÎπÑÎîîÏò§ÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Ï∂îÏ∂úÌïòÏó¨, DataframeÏùÑ ÎßåÎì§Í∏∞ ÏúÑÌï¥ Îπà listÎ•º ÏÉùÏÑ±ÌïúÎã§.\n",
    "title = []\n",
    "views = []\n",
    "likes = []\n",
    "# dislikes = []\n",
    "comments = []\n",
    "upload_date = []\n",
    "print('>>> Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ï§ÄÎπÑ ÏôÑÎ£å')\n",
    "\n",
    "# Í∞Å ÎπÑÎîîÏò§ÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Í∞ÄÏ†∏ÏôÄÏÑú Î¶¨Ïä§Ìä∏Ïóê Ï∂îÍ∞ÄÌïúÎã§.\n",
    "print('>>> Í∞úÎ≥Ñ ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏãúÏûë')\n",
    "for i in range(len(videoid_list)):\n",
    "    # for i in range(200):\n",
    "    request = youtube.videos().list(part='snippet,contentDetails,statistics', id=videoid_list[i])\n",
    "    response = request.execute()\n",
    "\n",
    "    if response['items'] == []:\n",
    "        title.append('-')\n",
    "        views.append('-')\n",
    "        likes.append('-')\n",
    "        # dislikes.append('-')\n",
    "        comments.append('-')\n",
    "\n",
    "    else:\n",
    "        # resultÏóêÏÑú Ï∂îÏ∂ú\n",
    "        tname = response['items'][0]['snippet']['title']\n",
    "        vc = response['items'][0]['statistics']['viewCount']\n",
    "        lc = response['items'][0]['statistics']['likeCount']\n",
    "        dlc = response['items'][0]['statistics']['dislikeCount']\n",
    "        cc = response['items'][0]['statistics']['commentCount']\n",
    "        pA = response['items'][0]['snippet']['publishedAt']\n",
    "\n",
    "        # append\n",
    "        title.append(tname)\n",
    "        views.append(vc)\n",
    "        likes.append(lc)\n",
    "        # dislikes.append(dlc)\n",
    "        comments.append(cc)\n",
    "        upload_date.append(pA)\n",
    "\n",
    "print('>>> Í∞úÎ≥Ñ ÎπÑÎîîÏò§ Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å')\n",
    "print('>>> ÎπÑÎîîÏò§ URL Ï†ïÎ¶¨')\n",
    "# YouTube API ÏùëÎãµÏóêÎäî Video URLÏù¥ ÏóÜÏùå. Ïù¥Î•º ÏÉùÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ prefix + Video IDÎ°ú Î¶¨Ïä§Ìä∏Î•º ÎßåÎì†Îã§.\n",
    "vidurl_prefix = 'https://www.youtube.com/watch?v='\n",
    "vidurl_list = []\n",
    "\n",
    "for i in range(len(videoid_list)):\n",
    "    vidurl = vidurl_prefix + videoid_list[i]\n",
    "    vidurl_list.append(vidurl)\n",
    "\n",
    "# Google APIÏùò ÏùëÎãµÏùÄ UTCÎ•º Í∏∞Ï§ÄÏúºÎ°ú ÌïúÎã§. KSTÎ°ú Î≥ÄÌôòÏù¥ ÌïÑÏöîÌïòÎ©∞, KSTÎäî UTC+9Ïù¥Îã§.\n",
    "\n",
    "original_pubdate = []\n",
    "for i in range(len(upload_date)):\n",
    "    originaldate = upload_date[i]\n",
    "    convertedtime = datetime.datetime.strptime(originaldate, '%Y-%m-%dT%H:%M:%SZ')\n",
    "    KSTdate = datetime.datetime.strptime(originaldate, '%Y-%m-%dT%H:%M:%SZ') + datetime.timedelta(hours=9)\n",
    "    KST_converted = KSTdate.strftime('%Y-%m-%d %H:%M')\n",
    "    original_pubdate.append(KST_converted)\n",
    "\n",
    "\n",
    "# ÏúÑÏóêÍπåÏßÄ ÏÉùÏÑ±Îêú Î™®Îì† Î¶¨Ïä§Ìä∏Î•º ÌïòÎÇòÏùò Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú ÏòÆÍ∏¥Îã§.\n",
    "print('>>> Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÌòïÌÉúÎ°ú Í∞ÄÍ≥µ')\n",
    "sum_df = pd.DataFrame([title, original_pubdate, videoid_list, vidurl_list, views, likes, comments]).T\n",
    "\n",
    "# Ìé∏ÏùòÎ•º ÏúÑÌï¥ Ïª¨Îüº Ïù¥Î¶ÑÏùÑ Ï∂îÍ∞ÄÌï¥Ï§ÄÎã§.\n",
    "sum_df.columns = ['title', 'PublishedAt', 'ID', 'URL', 'views', 'likes', 'comments']\n",
    "\n",
    "# Ïú†ÌäúÎ∏å Ï°∞ÌöåÏàòÎäî Îß§Ïùº Îã§Î•¥ÎØÄÎ°ú, Ïò§Îäò ÏûëÏóÖ ÎÇ†ÏßúÎ•º Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏúºÎ°ú Ï∂îÍ∞ÄÌïúÎã§. ÏãúÍ∞ÑÏùÄ Î¨¥ÏãúÌïúÎã§.\n",
    "# Îç∞Ïù¥ÌÑ∞ ÌîÑÎ†àÏûÑÏóê ÎÑ£Í∏∞ Ï†ÑÏóê, ÎπÑÎîîÏò§ Í∞úÏàòÎßåÌÅº ÎÇ†ÏßúÍ∞Ä Îì§Ïñ¥Í∞Ñ Î¶¨Ïä§Ìä∏Î•º ÎßåÎì†Îã§.\n",
    "date_list = []\n",
    "for i in range(len(videoid_list)):\n",
    "    date_list.append(nowDate)\n",
    "\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê 'Last_update_Date'ÏùÑ Ï∂îÍ∞ÄÌïúÎã§.\n",
    "print('>>> Ïò§Îäò ÎÇ†Ïßú(ÏûëÏóÖÏùº) Í∏∞Î°ù Ï§ë')\n",
    "sum_df['Last_updated_Date'] = date_list\n",
    "\n",
    "# Ï±ÑÎÑêÎ™ÖÏùÑ Îã§Ïãú Í∞ÄÏ†∏Ïò®Îã§.\n",
    "res2 = youtube.channels().list(id=chan_id, part='snippet').execute()\n",
    "channel_title = res2['items'][0]['snippet']['title']\n",
    "\n",
    "# Ïò§Îäò ÎÇ†ÏßúÍ∞Ä Îì§Ïñ¥Í∞Ñ csv ÌååÏùºÏùÑ ÏÉùÏÑ±ÌïúÎã§.\n",
    "print('>>> ÏûëÏóÖÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§.')\n",
    "filename = channel_title + '_' + today.strftime('%Y%m%d') + '.csv'\n",
    "sum_df.to_csv(filename, encoding='utf-8-sig', index=False)\n",
    "print('Í≤∞Í≥ºÎ¨º: ',filename)\n",
    "# CHANNEL_ID = \"UCoVz66yWHzVsXAFG8WhJK9g\"  # KBO Í≥µÏãù Ï±ÑÎÑê ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
